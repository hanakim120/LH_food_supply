{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "def process(config):\n",
    "    train_df = pd.read_csv('../data/train.csv', encoding='utf-8')\n",
    "    test_df = pd.read_csv('../data/test.csv', encoding='utf-8')\n",
    "    sample = pd.read_csv('../data/sample_submission.csv', encoding='utf-8')\n",
    "\n",
    "    y = train_df[['중식계', '석식계']]\n",
    "\n",
    "    TRAIN_LENGTH = 1205\n",
    "\n",
    "    df = pd.concat([train_df, test_df], axis=0)\n",
    "    df['출근'] = df['본사정원수'] - (df['본사휴가자수'] + df['본사출장자수'] + df['현본사소속재택근무자수'])\n",
    "    df['휴가비율'] = df['본사휴가자수'] / df['본사정원수']\n",
    "    df['출장비율'] = df['본사출장자수'] / df['본사정원수']\n",
    "    df['야근비율'] = df['본사시간외근무명령서승인건수'] / df['출근']\n",
    "    df['재택비율'] = df['현본사소속재택근무자수'] / df['본사정원수']\n",
    "\n",
    "    df.drop(columns=['본사정원수', '본사휴가자수', '본사출장자수', '현본사소속재택근무자수'], inplace=True)\n",
    "\n",
    "    df['공휴일전후'] = 0\n",
    "    df['공휴일전후'][17] = 1\n",
    "    df['공휴일전후'][3] = 1\n",
    "    df['공휴일전후'][62] = 1\n",
    "    df['공휴일전후'][131] = 1\n",
    "    df['공휴일전후'][152] = 1\n",
    "    df['공휴일전후'][226] = 1\n",
    "    df['공휴일전후'][221] = 1\n",
    "    df['공휴일전후'][224] = 1\n",
    "    df['공휴일전후'][245] = 1\n",
    "    df['공휴일전후'][310] = 2\n",
    "    df['공휴일전후'][311] = 1\n",
    "    df['공휴일전후'][309] = 1\n",
    "    df['공휴일전후'][330] = 1\n",
    "    df['공휴일전후'][379] = 1\n",
    "    df['공휴일전후'][467] = 1\n",
    "    df['공휴일전후'][470] = 1\n",
    "    df['공휴일전후'][502] = 2\n",
    "    df['공휴일전후'][565] = 1\n",
    "    df['공휴일전후'][623] = 1\n",
    "    df['공휴일전후'][651] = 1\n",
    "    df['공휴일전후'][705] = 1\n",
    "    df['공휴일전후'][709] = 1\n",
    "    df['공휴일전후'][815] = 1\n",
    "    df['공휴일전후'][864] = 1\n",
    "    df['공휴일전후'][950] = 1\n",
    "    df['공휴일전후'][951] = 1\n",
    "    df['공휴일전후'][953] = 1\n",
    "    df['공휴일전후'][955] = 1\n",
    "    df['공휴일전후'][954] = 1\n",
    "    df['공휴일전후'][971] = 1\n",
    "    df['공휴일전후'][1038] = 1\n",
    "    df['공휴일전후'][1099] = 1\n",
    "    df['공휴일전후'][1129] = 1\n",
    "    df['공휴일전후'][1187] = 1\n",
    "\n",
    "    df['공휴일전후'][TRAIN_LENGTH + 10] = 1\n",
    "    df['공휴일전후'][TRAIN_LENGTH + 20] = 1\n",
    "\n",
    "    df = pd.get_dummies(df, columns=['공휴일전후'])\n",
    "    df['공휴일전후_0'][TRAIN_LENGTH + 20] = 1\n",
    "    df['공휴일전후_1'][TRAIN_LENGTH + 20] = 0\n",
    "\n",
    "    if config.temp:\n",
    "        temp = pd.read_csv('../data/temp.csv', encoding='utf-8')\n",
    "        df = pd.merge(df, temp, on='일자')\n",
    "\n",
    "    if config.sep_date:\n",
    "        df['year'] = df.일자.apply(lambda x : int(x[:4]))\n",
    "        df['month'] = df.일자.apply(lambda x : int(x[-5 :-3]))\n",
    "        df['day'] = df.일자.apply(lambda x : int(x[-2 :]))\n",
    "        df['week'] = df.day.apply(lambda x : x // 7)\n",
    "\n",
    "        df.drop(columns=['일자'], inplace=True)\n",
    "    else :\n",
    "        df.일자 = pd.to_datetime(df.일자)\n",
    "        df.rename(columns={'일자':'ds'}, inplace=True)\n",
    "\n",
    "    columns = ['조식메뉴', '중식메뉴', '석식메뉴']\n",
    "    for col in columns :\n",
    "        df[col] = df[col].str.replace('/', ' ')\n",
    "        df[col] = df[col].str.replace(r'([<]{1}[ㄱ-힣\\:\\,\\.\\/\\-A-Za-z 0-9]*[>]{1})', '')\n",
    "        df[col] = df[col].str.replace(r'([＜]{1}[ㄱ-힣\\:\\,\\.\\/\\-A-Za-z 0-9]*[＞]{1})', '')\n",
    "        df[col] = df[col].str.replace(r'([(]{1}[ㄱ-힣\\:\\,\\.\\/\\-A-Za-z 0-9]*[)]{1})', '')\n",
    "        df[col] = df[col].str.replace(r'[ ]{2, }', ' ')\n",
    "        df[col] = df[col].str.replace('\\(New\\)', '')\n",
    "        df[col] = df[col].str.replace('\\(NeW\\)', '')\n",
    "        df[col] = df[col].str.replace(r'[D]{1}', '')\n",
    "        df[col] = df[col].str.replace(r'[S]{1}', '')\n",
    "        df[col] = df[col].str.replace('\\(쌀:국내산,돈육:국내', '')\n",
    "        df[col] = df[col].str.replace('고추가루:중국산\\)', '')\n",
    "        df[col] = df[col].str.replace('*', ' ')\n",
    "        df[col] = df[col].str.replace('[(]만두 고추 통계란[)]', '')\n",
    "        df[col] = df[col].str.replace('[(]모둠튀김 양념장[)]', '')\n",
    "        df[col] = df[col].apply(lambda x : x.strip())\n",
    "\n",
    "    # Normalize\n",
    "    scaling_cols = ['본사시간외근무명령서승인건수', '출근', '휴가비율', '야근비율', '재택비율', '출장비율']\n",
    "    for col in scaling_cols :\n",
    "        ms = MinMaxScaler()\n",
    "        ms.fit(df[col][:TRAIN_LENGTH].values.reshape(-1, 1))\n",
    "        df[col] = ms.transform(df[col].values.reshape(-1, 1))\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df.요일.values[:TRAIN_LENGTH])\n",
    "    df.요일 = le.transform(df.요일.values)\n",
    "\n",
    "    np.random.seed(0)\n",
    "    idx = np.random.permutation(TRAIN_LENGTH)\n",
    "    train_idx = idx[:1000]\n",
    "    valid_idx = idx[1000:]\n",
    "\n",
    "    train_df = df.iloc[train_idx, :]\n",
    "    valid_df = df.iloc[valid_idx, :]\n",
    "    test_df = df.iloc[TRAIN_LENGTH:, :]\n",
    "    train_y = y.iloc[train_idx, :]\n",
    "    valid_y = y.iloc[valid_idx, :]\n",
    "\n",
    "    train_df.drop(columns=['중식계', '석식계'], inplace=True)\n",
    "    valid_df.drop(columns=['중식계', '석식계'], inplace=True)\n",
    "    test_df.drop(columns=['중식계', '석식계'], inplace=True)\n",
    "\n",
    "    if config.text == 'embedding' :\n",
    "        train_tmp, valid_tmp, test_tmp = embedding(config, train_df, valid_df, test_df)\n",
    "\n",
    "    # if config.text == 'subword':\n",
    "    #     tmp = subword(config, train_df, valid_df, test_df)\n",
    "\n",
    "    train_df = pd.concat([train_df.reset_index(drop=True), train_tmp.reset_index(drop=True)], axis=1)\n",
    "    train_df.drop(columns=['조식메뉴', '중식메뉴', '석식메뉴'], inplace=True)\n",
    "\n",
    "    valid_df = pd.concat([valid_df.reset_index(drop=True), valid_tmp.reset_index(drop=True)], axis=1)\n",
    "    valid_df.drop(columns=['조식메뉴', '중식메뉴', '석식메뉴'], inplace=True)\n",
    "\n",
    "    test_df = pd.concat([test_df.reset_index(drop=True), test_tmp.reset_index(drop=True)], axis=1)\n",
    "    test_df.drop(columns=['조식메뉴', '중식메뉴', '석식메뉴'], inplace=True)\n",
    "\n",
    "    print('|TRAIN| : {} |VALID| : {} |TEST| : {}'.format(train_df.shape, valid_df.shape, test_df.shape))\n",
    "    print(train_y.shape, valid_y.shape)\n",
    "    return train_df, valid_df, test_df, train_y, valid_y, sample\n",
    "\n",
    "\n",
    "def autoencoding(train_df, test_df, TARGET=1, verbose=2) :\n",
    "    TRAIN_LENGTH = 1205\n",
    "\n",
    "    df = pd.concat([train_df, test_df], axis=0)\n",
    "    text_df = df[['조식메뉴', '중식메뉴', '석식메뉴']]\n",
    "\n",
    "    mecab = Mecab(dicpath='C:/mecab/mecab-ko-dic')\n",
    "\n",
    "    for i in range(len(text_df.조식메뉴)) :\n",
    "        text_df.조식메뉴[i] = ','.join(text_df.조식메뉴[i].split())\n",
    "        text_df.조식메뉴[i] = ' '.join(mecab.morphs(text_df.조식메뉴[i]))\n",
    "    for i in range(len(text_df.중식메뉴)) :\n",
    "        text_df.중식메뉴[i] = ','.join(text_df.중식메뉴[i].split())\n",
    "        text_df.중식메뉴[i] = ' '.join(mecab.morphs(text_df.중식메뉴[i]))\n",
    "    for i in range(len(text_df.석식메뉴)) :\n",
    "        text_df.석식메뉴[i] = ','.join(text_df.석식메뉴[i].split())\n",
    "        text_df.석식메뉴[i] = ' '.join(mecab.morphs(text_df.석식메뉴[i]))\n",
    "\n",
    "    vect = CountVectorizer()\n",
    "    vect.fit(text_df['조식메뉴'].values[:TRAIN_LENGTH])\n",
    "    breakfast = vect.transform(text_df['조식메뉴'].values).toarray()\n",
    "    vect.fit(text_df['중식메뉴'].values[:TRAIN_LENGTH])\n",
    "    lunch = vect.transform(text_df['중식메뉴'].values).toarray()\n",
    "    vect.fit(text_df['석식메뉴'].values[:TRAIN_LENGTH])\n",
    "    dinner = vect.transform(text_df['석식메뉴'].values).toarray()\n",
    "\n",
    "    enc_df = pd.DataFrame()\n",
    "\n",
    "    i = 1\n",
    "    for menu in [breakfast, lunch, dinner] :\n",
    "        print('+' * 10, 'Train {}'.format(i), '+' * 10)\n",
    "        train_X, valid_X = train_test_split(menu[:TRAIN_LENGTH],\n",
    "                                            test_size=0.1,\n",
    "                                            shuffle=True,\n",
    "                                            random_state=0)\n",
    "        STEP = (menu.shape[1] - TARGET) // 5\n",
    "        INPUT = menu.shape[1]\n",
    "\n",
    "        model = AutoEncoder(INPUT, STEP)\n",
    "        model.compile(loss='mse', optimizer='adam', metrics='mse')\n",
    "        model.fit(train_X, train_X,\n",
    "                  validation_data=(valid_X, valid_X),\n",
    "                  epochs=1000,\n",
    "                  callbacks=tf.keras.callbacks.EarlyStopping(patience=10),\n",
    "                  verbose=verbose)\n",
    "        result = model.encoder(menu)\n",
    "        result = np.array(result)\n",
    "\n",
    "        enc_df = pd.concat([enc_df, pd.DataFrame(result)], axis=1)\n",
    "        i += 1\n",
    "        print()\n",
    "\n",
    "    train_df.drop(columns=['조식메뉴', '중식메뉴', '석식메뉴'], inplace=True)\n",
    "    test_df.drop(columns=['조식메뉴', '중식메뉴', '석식메뉴'], inplace=True)\n",
    "\n",
    "    train_df_enc = pd.concat([train_df, enc_df[:TRAIN_LENGTH]], axis=1)\n",
    "    test_df_enc = pd.concat([test_df, enc_df[TRAIN_LENGTH :]], axis=1)\n",
    "\n",
    "    column_names = list(train_df_enc.columns[:8]) + [i for i in range(enc_df.shape[1])]\n",
    "    train_df_enc.columns = column_names\n",
    "    test_df_enc.columns = column_names\n",
    "\n",
    "    return train_df_enc, test_df_enc\n",
    "\n",
    "\n",
    "def get_data(config) :\n",
    "    train_df, valid_df, test_df, train_y, valid_y, sample = process(config)\n",
    "    if config.text == 'autoencode' :\n",
    "        train_df, test_df = autoencoding(train_df, test_df, TARGET=config.dim)\n",
    "\n",
    "    return train_df, valid_df, test_df, train_y, valid_y, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from argparse import Namespace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import fasttext\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "\n",
    "TRAIN_LENGTH = 1205\n",
    "\n",
    "\n",
    "def pretext(text) :\n",
    "    text = text.rstrip()\n",
    "    text = re.sub('▁', '', text)\n",
    "    return text\n",
    "\n",
    "def get_fasttext_model(config, train_df, save=False):\n",
    "    print('\"{}\" DOES NOT EXIST. START TRAINING MODEL'.format(config.fasttext_model_fn))\n",
    "    corpus = np.concatenate([train_df.조식메뉴.values,\n",
    "                             train_df.중식메뉴.values,\n",
    "                             train_df.석식메뉴.values], axis=0)\n",
    "    with open('./data/corpus.train.txt', 'w', -1, encoding='utf-8') as f :\n",
    "        f.write('\\n'.join(corpus))\n",
    "\n",
    "    if config.dummy_corpus :\n",
    "        args = {'load_fn' : './data/corpus.train.txt',\n",
    "                'save_fn' : './data/corpus.train.dummy.txt',\n",
    "                'iter' : 2,\n",
    "                'verbose' : 300}\n",
    "        args = Namespace(**args)\n",
    "        main(args)\n",
    "\n",
    "    model = fasttext.train_unsupervised(\n",
    "        './data/corpus.train.dummy.txt' if config.dummy_corpus else './data/corpus.train.txt',\n",
    "        dim=config.dim,\n",
    "        ws=config.window_size,\n",
    "        epoch=config.fasttext_epoch,\n",
    "        min_count=config.min_count,\n",
    "        minn=config.min_ngram,\n",
    "        maxn=config.max_ngram,\n",
    "        )\n",
    "    if save:\n",
    "        model.save_model('./data/{}'.format(config.fasttext_model_fn))\n",
    "    return model\n",
    "\n",
    "def embedding(config, train_df, valid_df, test_df):\n",
    "    embedding_features = []\n",
    "\n",
    "    if config.pretrained:\n",
    "        print('USE PRETRAINED MODEL: cc.ko.300.bin')\n",
    "        model = fasttext.load_model('cc.ko.300.bin')\n",
    "        fasttext.util.reduce_model(model, config.dim)\n",
    "\n",
    "    else:\n",
    "        if config.fasttext_model_fn is not None:\n",
    "            try:\n",
    "                model = fasttext.load_model('./data/{}'.format(config.fasttext_model_fn))\n",
    "            except:\n",
    "                model = get_fasttext_model(config, train_df, save=True)\n",
    "        else:\n",
    "            model = get_fasttext_model(config, train_df, save=False)\n",
    "\n",
    "\n",
    "    TRAIN_LENGTH, VALID_LENGTH = train_df.shape[0], valid_df.shape[0]\n",
    "\n",
    "    df = pd.concat([train_df, valid_df, test_df], axis=0)\n",
    "    breakfast = df.조식메뉴.values\n",
    "    lunch = df.중식메뉴.values\n",
    "    dinner = df.석식메뉴.values\n",
    "\n",
    "    breakfast_array = np.zeros((1255, config.dim))\n",
    "    lunch_array = np.zeros((1255, config.dim))\n",
    "    dinner_array = np.zeros((1255, config.dim))\n",
    "    for i in range(1255) :\n",
    "\n",
    "        breakfast_array[i] = model.get_sentence_vector(breakfast[i]) / (len(breakfast[i].split()) + 1)\n",
    "        lunch_array[i] = model.get_sentence_vector(lunch[i]) / (len(lunch[i].split()) + 1)\n",
    "        dinner_array[i] = model.get_sentence_vector(dinner[i]) / (len(dinner[i].split()) + 1)\n",
    "\n",
    "    for i in range(config.dim) :\n",
    "        embedding_features.append('breakfast_{}'.format(i))\n",
    "        embedding_features.append('lunch_{}'.format(i))\n",
    "        embedding_features.append('dinner_{}'.format(i))\n",
    "\n",
    "    tmp = pd.concat([\n",
    "        pd.DataFrame(breakfast_array, columns=['breakfast_{}'.format(i) for i in range(config.dim)]),\n",
    "        pd.DataFrame(lunch_array, columns=['lunch_{}'.format(i) for i in range(config.dim)]),\n",
    "        pd.DataFrame(dinner_array, columns=['dinner_{}'.format(i) for i in range(config.dim)])], axis=1)\n",
    "    train_tmp = tmp[:TRAIN_LENGTH]\n",
    "    valid_tmp = tmp[TRAIN_LENGTH: TRAIN_LENGTH + VALID_LENGTH]\n",
    "    test_tmp = tmp[TRAIN_LENGTH + VALID_LENGTH:]\n",
    "\n",
    "    return train_tmp, valid_tmp, test_tmp\n",
    "\n",
    "# def subword(config):\n",
    "#     # corpus = '\\n'.join(np.concatenate([breakfast[:TRAIN_LENGTH],\n",
    "#     #                                    lunch[:TRAIN_LENGTH],\n",
    "#     #                                    dinner[:TRAIN_LENGTH]], axis=0))\n",
    "#     # with open('./data/corpus.train.txt', 'w', encoding='utf-8') as f:\n",
    "#     #     f.write(corpus)\n",
    "#     #\n",
    "#     # corpus = '\\n'.join(np.concatenate([breakfast[TRAIN_LENGTH:],\n",
    "#     #                                    lunch[TRAIN_LENGTH:],\n",
    "#     #                                    dinner[TRAIN_LENGTH:]], axis=0))\n",
    "#     # with open('./data/corpus.test.txt', 'w', encoding='utf-8') as f:\n",
    "#     #     f.write(corpus)\n",
    "#\n",
    "#     # =================== subword segment=========================\n",
    "#     with open('./data/corpus.train.sub.txt', 'r', encoding='utf-8') as f :\n",
    "#         data = f.readlines()\n",
    "#     corpus_sub_train = list(map(pretext, data))\n",
    "#     with open('./data/corpus.valid.sub.txt', 'r', encoding='utf-8') as f :\n",
    "#         data = f.readlines()\n",
    "#     corpus_sub_valid = list(map(pretext, data))\n",
    "#     with open('./data/corpus.test.sub.txt', 'r', encoding='utf-8') as f :\n",
    "#         data = f.readlines()\n",
    "#     corpus_sub_test = list(map(pretext, data))\n",
    "#\n",
    "#     tokenizer = Tokenizer(oov_token='<oov>')\n",
    "#     tokenizer.fit_on_texts(corpus_sub_train)\n",
    "#\n",
    "#     vocab_size = len(tokenizer.word_index)\n",
    "#     print('VOCAB SIZE : {}'.format(vocab_size))\n",
    "#     cnt = 0\n",
    "#     for i in tokenizer.word_counts.values():\n",
    "#         if i == 1:\n",
    "#             cnt += 1\n",
    "#     print('Freq 1 word : {}'.format(cnt))\n",
    "#     tokenizer = Tokenizer(oov_token='<oov>', num_words=vocab_size \\\n",
    "#         if not config.sub_sparse_word \\\n",
    "#         else vocab_size - cnt + 2)\n",
    "#\n",
    "#     tokenizer.fit_on_texts(corpus_sub_train)\n",
    "#\n",
    "#     corpus_sub = corpus_sub_train + corpus_sub_valid + corpus_sub_test\n",
    "#     corpus_seq = tokenizer.texts_to_sequences(corpus_sub)\n",
    "#\n",
    "#     embeds = nn.Embedding(839, config.dim)\n",
    "#\n",
    "#     def get_sentence_vect(seq) :\n",
    "#         result = torch.tensor(torch.zeros((config.dim)))\n",
    "#         for idx in seq :\n",
    "#             result += embeds(torch.tensor(idx, dtype=torch.long))\n",
    "#         return (result / len(seq)).detach().numpy()\n",
    "#\n",
    "#     tmp = pd.DataFrame(np.array(list(map(get_sentence_vect, corpus_seq))))\n",
    "#     tmp = pd.concat([tmp.iloc[:1255, :],\n",
    "#                      tmp.iloc[1255 :2510, :].reset_index(drop=True),\n",
    "#                      tmp.iloc[2510 :, :].reset_index(drop=True)], axis=1)\n",
    "#     columns = []\n",
    "#\n",
    "#     for menu in ['breakfast', 'lunch', 'dinner']:\n",
    "#         for i in range(config.dim):\n",
    "#             columns.append('{}_{}'.format(menu, i))\n",
    "#\n",
    "#     tmp.columns = columns\n",
    "#\n",
    "#     return tmp\n",
    "\n",
    "class Encoder(tf.keras.models.Model) :\n",
    "    def __init__(self, step, input_size) :\n",
    "        super(Encoder, self).__init__()\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "            InputLayer(input_shape=(input_size,)),\n",
    "            Dense(input_size - step * 1, activation='relu'),\n",
    "            Dense(input_size - step * 2, activation='relu'),\n",
    "            Dense(input_size - step * 3, activation='relu'),\n",
    "            Dense(input_size - step * 4, activation='relu'),\n",
    "            Dense(input_size - step * 5),\n",
    "        ])\n",
    "\n",
    "    def call(self, x) :\n",
    "        z = self.model(x)\n",
    "        return z\n",
    "\n",
    "class Decoder(tf.keras.models.Model) :\n",
    "    def __init__(self, step, input_size, output_size) :\n",
    "        super(Decoder, self).__init__()\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "            InputLayer(input_shape=(input_size,)),\n",
    "            Dense(output_size - step * 4, activation='relu'),\n",
    "            #             Dense(output_size - step * 3, activation='relu'),\n",
    "            Dense(output_size - step * 2, activation='relu'),\n",
    "            #             Dense(output_size - step * 1, activation='relu'),\n",
    "            Dense(output_size),\n",
    "        ])\n",
    "\n",
    "    def call(self, x) :\n",
    "        z = self.model(x)\n",
    "        return z\n",
    "\n",
    "class AutoEncoder(tf.keras.models.Model) :\n",
    "    def __init__(self, input_size, step) :\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder(step, input_size)\n",
    "        self.decoder = Decoder(step, input_size - step * 5, input_size)\n",
    "\n",
    "    def call(self, x) :\n",
    "        y = self.encoder(x)\n",
    "        z = self.decoder(y)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"basemodel.bin\" DOES NOT EXIST. START TRAINING MODEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/corpus.train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-403f491236e6>\u001b[0m in \u001b[0;36membedding\u001b[1;34m(config, train_df, valid_df, test_df)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfasttext_model_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\fasttext\\FastText.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[0meprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_FastText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\fasttext\\FastText.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_path, args)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: ./data/basemodel.bin cannot be opened for loading!",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-9a97fe589cb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m          }\n\u001b[0;32m      9\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNamespace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mvalid_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalid_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-c50ac0b821fa>\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m     \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'autoencode'\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautoencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTARGET\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-c50ac0b821fa>\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'embedding'\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mtrain_tmp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_tmp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_tmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;31m# if config.text == 'subword':\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-403f491236e6>\u001b[0m in \u001b[0;36membedding\u001b[1;34m(config, train_df, valid_df, test_df)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfasttext_model_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_fasttext_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_fasttext_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-403f491236e6>\u001b[0m in \u001b[0;36mget_fasttext_model\u001b[1;34m(config, train_df, save)\u001b[0m\n\u001b[0;32m     25\u001b[0m                              \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m중식메뉴\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                              train_df.석식메뉴.values], axis=0)\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/corpus.train.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/corpus.train.txt'"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "config = {'temp':True,\n",
    "          'sep_date':True,\n",
    "          'text':'embedding',\n",
    "          'pretrained':False,\n",
    "          'fasttext_model_fn':'basemodel.bin',\n",
    "          'dim':3,\n",
    "         }\n",
    "config = Namespace(**config)\n",
    "train_df, valid_df, test_df, train_y, valid_y, sample = get_data(config)\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_y.reset_index(drop=True)], axis=0)\n",
    "valid_df = pd.concat([valid_df.reset_index(drop=True), valid_y.reset_index(drop=True)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column type forced is either target column or doesn't exist in the dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-ccfffc540d95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mFOLD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m reg = setup(data=train_df.drop(columns=['석식계']), target='중식계', test_data=valid_df.drop(columns=['석식계']),\n\u001b[0m\u001b[0;32m      5\u001b[0m             categorical_features=[0, 7, 8, 9, 12, 13, 14, 15], fold=FOLD)\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pycaret\\regression.py\u001b[0m in \u001b[0;36msetup\u001b[1;34m(data, target, train_size, test_data, preprocess, imputation_type, iterative_imputation_iters, categorical_features, categorical_imputation, categorical_iterative_imputer, ordinal_features, high_cardinality_features, high_cardinality_method, numeric_features, numeric_imputation, numeric_iterative_imputer, date_features, ignore_features, normalize, normalize_method, transformation, transformation_method, handle_unknown_categorical, unknown_categorical_method, pca, pca_method, pca_components, ignore_low_variance, combine_rare_levels, rare_level_threshold, bin_numeric_features, remove_outliers, outliers_threshold, remove_multicollinearity, multicollinearity_threshold, remove_perfect_collinearity, create_clusters, cluster_iter, polynomial_features, polynomial_degree, trigonometry_features, polynomial_threshold, group_features, group_names, feature_selection, feature_selection_threshold, feature_selection_method, feature_interaction, feature_ratio, interaction_threshold, transform_target, transform_target_method, data_split_shuffle, data_split_stratify, fold_strategy, fold, fold_shuffle, fold_groups, n_jobs, use_gpu, custom_pipeline, html, session_id, log_experiment, experiment_name, log_plots, log_profile, log_data, silent, verbose, profile)\u001b[0m\n\u001b[0;32m    563\u001b[0m         \u001b[0mlog_plots\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"residuals\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"error\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"feature\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m     return pycaret.internal.tabular.setup(\n\u001b[0m\u001b[0;32m    566\u001b[0m         \u001b[0mml_usecase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"regression\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[0mavailable_plots\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mavailable_plots\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pycaret\\internal\\tabular.py\u001b[0m in \u001b[0;36msetup\u001b[1;34m(data, target, ml_usecase, available_plots, train_size, test_data, preprocess, imputation_type, iterative_imputation_iters, categorical_features, categorical_imputation, categorical_iterative_imputer, ordinal_features, high_cardinality_features, high_cardinality_method, numeric_features, numeric_imputation, numeric_iterative_imputer, date_features, ignore_features, normalize, normalize_method, transformation, transformation_method, handle_unknown_categorical, unknown_categorical_method, pca, pca_method, pca_components, ignore_low_variance, combine_rare_levels, rare_level_threshold, bin_numeric_features, remove_outliers, outliers_threshold, remove_multicollinearity, multicollinearity_threshold, remove_perfect_collinearity, create_clusters, cluster_iter, polynomial_features, polynomial_degree, trigonometry_features, polynomial_threshold, group_features, group_names, feature_selection, feature_selection_threshold, feature_selection_method, feature_interaction, feature_ratio, interaction_threshold, fix_imbalance, fix_imbalance_method, transform_target, transform_target_method, data_split_shuffle, data_split_stratify, fold_strategy, fold, fold_shuffle, fold_groups, n_jobs, use_gpu, custom_pipeline, html, session_id, log_experiment, experiment_name, log_plots, log_profile, log_data, silent, verbose, profile, display)\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_cols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    593\u001b[0m                     \u001b[1;34m\"Column type forced is either target column or doesn't exist in the dataset.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m                 )\n",
      "\u001b[1;31mValueError\u001b[0m: Column type forced is either target column or doesn't exist in the dataset."
     ]
    }
   ],
   "source": [
    "from pycaret.regression import *\n",
    "\n",
    "FOLD = 10\n",
    "reg = setup(data=train_df.drop(columns=['석식계']), target='중식계', test_data=valid_df.drop(columns=['석식계']),\n",
    "            categorical_features=[0, 7, 8, 9, 12, 13, 14, 15], fold=FOLD)\n",
    "\n",
    "print('=' * 10, 'Comparing models...', '=' * 10)\n",
    "best5 = compare_models(fold=FOLD, sort='mae', n_select=5)\n",
    "\n",
    "print('=' * 10, 'Tuning models', '=' * 10)\n",
    "tuned_best5 = [tune_model(i, optimize='mae', \n",
    "                          early_stopping=True, \n",
    "                          early_stopping_max_iters=100,\n",
    "                          choose_better=True) for i in best5]\n",
    "print('=' * 10, 'Blending...', '=' * 10)\n",
    "blend_best5 = blend_models(estimator_list=tuned_best5, fold=FOLD, optimize='mae',\n",
    "                           choose_better=True)\n",
    "\n",
    "print('=' * 10, 'Finalizing...', '=' * 10)\n",
    "pred = predict_model(blend_best5)\n",
    "final_model = finalize_model(blend_best5)\n",
    "pred = predict_model(final_model, data=test_df)\n",
    "print('=' * 10, 'Completed!', '=' * 10)\n",
    "sample['중식계'] = pred.Label.values\n",
    "\n",
    "\n",
    "reg = setup(data=train_df.drop(columns=['중식계']), target='석식계', test_data=valid_df.drop(columns=['중식계']),\n",
    "            categorical_features=[0, 7, 8, 9, 12, 13, 14, 15], fold=FOLD)\n",
    "\n",
    "print('=' * 10, 'Comparing models...', '=' * 10)\n",
    "best5 = compare_models(fold=FOLD, sort='mae', n_select=5)\n",
    "\n",
    "print('=' * 10, 'Tuning models', '=' * 10)\n",
    "tuned_best5 = [tune_model(i, optimize='mae', \n",
    "                          early_stopping=True, \n",
    "                          early_stopping_max_iters=100,\n",
    "                          choose_better=True) for i in best5]\n",
    "print('=' * 10, 'Blending...', '=' * 10)\n",
    "blend_best5 = blend_models(estimator_list=tuned_best5, fold=FOLD, optimize='mae',\n",
    "                           choose_better=True)\n",
    "\n",
    "print('=' * 10, 'Finalizing...', '=' * 10)\n",
    "pred = predict_model(blend_best5)\n",
    "final_model = finalize_model(blend_best5)\n",
    "pred = predict_model(final_model, data=test_df)\n",
    "print('=' * 10, 'Completed!', '=' * 10)\n",
    "sample['석식계'] = pred.Label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv('submission_pycaret.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submission_pycaret.csv\n",
    "- 5dim fasttext / n_select 2\n",
    "- 10dim fasttext / n_select 3 / normalize / multicollinearity ths 0.9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
