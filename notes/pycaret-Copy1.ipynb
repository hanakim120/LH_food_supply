{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "def process(config):\n",
    "    train_df = pd.read_csv('../data/train.csv', encoding='utf-8')\n",
    "    test_df = pd.read_csv('../data/test.csv', encoding='utf-8')\n",
    "    sample = pd.read_csv('../data/sample_submission.csv', encoding='utf-8')\n",
    "\n",
    "    y = train_df[['중식계', '석식계']]\n",
    "\n",
    "    TRAIN_LENGTH = 1205\n",
    "\n",
    "    df = pd.concat([train_df, test_df], axis=0)\n",
    "    df['출근'] = df['본사정원수'] - (df['본사휴가자수'] + df['본사출장자수'] + df['현본사소속재택근무자수'])\n",
    "    df['휴가비율'] = df['본사휴가자수'] / df['본사정원수']\n",
    "    df['출장비율'] = df['본사출장자수'] / df['본사정원수']\n",
    "    df['야근비율'] = df['본사시간외근무명령서승인건수'] / df['출근']\n",
    "    df['재택비율'] = df['현본사소속재택근무자수'] / df['본사정원수']\n",
    "\n",
    "    df.drop(columns=['본사정원수', '본사휴가자수', '본사출장자수', '현본사소속재택근무자수'], inplace=True)\n",
    "\n",
    "    df['공휴일전후'] = 0\n",
    "    df['공휴일전후'][17] = 1\n",
    "    df['공휴일전후'][3] = 1\n",
    "    df['공휴일전후'][62] = 1\n",
    "    df['공휴일전후'][131] = 1\n",
    "    df['공휴일전후'][152] = 1\n",
    "    df['공휴일전후'][226] = 1\n",
    "    df['공휴일전후'][221] = 1\n",
    "    df['공휴일전후'][224] = 1\n",
    "    df['공휴일전후'][245] = 1\n",
    "    df['공휴일전후'][310] = 2\n",
    "    df['공휴일전후'][311] = 1\n",
    "    df['공휴일전후'][309] = 1\n",
    "    df['공휴일전후'][330] = 1\n",
    "    df['공휴일전후'][379] = 1\n",
    "    df['공휴일전후'][467] = 1\n",
    "    df['공휴일전후'][470] = 1\n",
    "    df['공휴일전후'][502] = 2\n",
    "    df['공휴일전후'][565] = 1\n",
    "    df['공휴일전후'][623] = 1\n",
    "    df['공휴일전후'][651] = 1\n",
    "    df['공휴일전후'][705] = 1\n",
    "    df['공휴일전후'][709] = 1\n",
    "    df['공휴일전후'][815] = 1\n",
    "    df['공휴일전후'][864] = 1\n",
    "    df['공휴일전후'][950] = 1\n",
    "    df['공휴일전후'][951] = 1\n",
    "    df['공휴일전후'][953] = 1\n",
    "    df['공휴일전후'][955] = 1\n",
    "    df['공휴일전후'][954] = 1\n",
    "    df['공휴일전후'][971] = 1\n",
    "    df['공휴일전후'][1038] = 1\n",
    "    df['공휴일전후'][1099] = 1\n",
    "    df['공휴일전후'][1129] = 1\n",
    "    df['공휴일전후'][1187] = 1\n",
    "\n",
    "    df['공휴일전후'][TRAIN_LENGTH + 10] = 1\n",
    "    df['공휴일전후'][TRAIN_LENGTH + 20] = 1\n",
    "\n",
    "    df = pd.get_dummies(df, columns=['공휴일전후'])\n",
    "    df['공휴일전후_0'][TRAIN_LENGTH + 20] = 1\n",
    "    df['공휴일전후_1'][TRAIN_LENGTH + 20] = 0\n",
    "\n",
    "    if config.temp:\n",
    "        temp = pd.read_csv('../data/temp.csv', encoding='utf-8')\n",
    "        df = pd.merge(df, temp, on='일자')\n",
    "\n",
    "    if config.sep_date:\n",
    "        df['year'] = df.일자.apply(lambda x : int(x[:4]))\n",
    "        df['month'] = df.일자.apply(lambda x : int(x[-5 :-3]))\n",
    "        df['day'] = df.일자.apply(lambda x : int(x[-2 :]))\n",
    "        df['week'] = df.day.apply(lambda x : x // 7)\n",
    "\n",
    "        df.drop(columns=['일자'], inplace=True)\n",
    "    else :\n",
    "        df.일자 = pd.to_datetime(df.일자)\n",
    "        df.rename(columns={'일자':'ds'}, inplace=True)\n",
    "\n",
    "    columns = ['조식메뉴', '중식메뉴', '석식메뉴']\n",
    "    for col in columns :\n",
    "        df[col] = df[col].str.replace('/', ' ')\n",
    "        df[col] = df[col].str.replace(r'([<]{1}[ㄱ-힣\\:\\,\\.\\/\\-A-Za-z 0-9]*[>]{1})', '')\n",
    "        df[col] = df[col].str.replace(r'([＜]{1}[ㄱ-힣\\:\\,\\.\\/\\-A-Za-z 0-9]*[＞]{1})', '')\n",
    "        df[col] = df[col].str.replace(r'([(]{1}[ㄱ-힣\\:\\,\\.\\/\\-A-Za-z 0-9]*[)]{1})', '')\n",
    "        df[col] = df[col].str.replace(r'[ ]{2, }', ' ')\n",
    "        df[col] = df[col].str.replace('\\(New\\)', '')\n",
    "        df[col] = df[col].str.replace('\\(NeW\\)', '')\n",
    "        df[col] = df[col].str.replace(r'[D]{1}', '')\n",
    "        df[col] = df[col].str.replace(r'[S]{1}', '')\n",
    "        df[col] = df[col].str.replace('\\(쌀:국내산,돈육:국내', '')\n",
    "        df[col] = df[col].str.replace('고추가루:중국산\\)', '')\n",
    "        df[col] = df[col].str.replace('*', ' ')\n",
    "        df[col] = df[col].str.replace('[(]만두 고추 통계란[)]', '')\n",
    "        df[col] = df[col].str.replace('[(]모둠튀김 양념장[)]', '')\n",
    "        df[col] = df[col].apply(lambda x : x.strip())\n",
    "\n",
    "    # Normalize\n",
    "    scaling_cols = ['본사시간외근무명령서승인건수', '출근', '휴가비율', '야근비율', '재택비율', '출장비율']\n",
    "    for col in scaling_cols :\n",
    "        ms = MinMaxScaler()\n",
    "        ms.fit(df[col][:TRAIN_LENGTH].values.reshape(-1, 1))\n",
    "        df[col] = ms.transform(df[col].values.reshape(-1, 1))\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df.요일.values[:TRAIN_LENGTH])\n",
    "    df.요일 = le.transform(df.요일.values)\n",
    "\n",
    "    np.random.seed(0)\n",
    "    idx = np.random.permutation(TRAIN_LENGTH)\n",
    "    train_idx = idx[:1000]\n",
    "    valid_idx = idx[1000:]\n",
    "\n",
    "    train_df = df.iloc[train_idx, :]\n",
    "    valid_df = df.iloc[valid_idx, :]\n",
    "    test_df = df.iloc[TRAIN_LENGTH:, :]\n",
    "    train_y = y.iloc[train_idx, :]\n",
    "    valid_y = y.iloc[valid_idx, :]\n",
    "\n",
    "    train_df.drop(columns=['중식계', '석식계'], inplace=True)\n",
    "    valid_df.drop(columns=['중식계', '석식계'], inplace=True)\n",
    "    test_df.drop(columns=['중식계', '석식계'], inplace=True)\n",
    "\n",
    "    if config.text == 'embedding' :\n",
    "        train_tmp, valid_tmp, test_tmp = embedding(config, train_df, valid_df, test_df)\n",
    "\n",
    "    # if config.text == 'subword':\n",
    "    #     tmp = subword(config, train_df, valid_df, test_df)\n",
    "\n",
    "    train_df = pd.concat([train_df.reset_index(drop=True), train_tmp.reset_index(drop=True)], axis=1)\n",
    "    train_df.drop(columns=['조식메뉴', '중식메뉴', '석식메뉴'], inplace=True)\n",
    "\n",
    "    valid_df = pd.concat([valid_df.reset_index(drop=True), valid_tmp.reset_index(drop=True)], axis=1)\n",
    "    valid_df.drop(columns=['조식메뉴', '중식메뉴', '석식메뉴'], inplace=True)\n",
    "\n",
    "    test_df = pd.concat([test_df.reset_index(drop=True), test_tmp.reset_index(drop=True)], axis=1)\n",
    "    test_df.drop(columns=['조식메뉴', '중식메뉴', '석식메뉴'], inplace=True)\n",
    "\n",
    "    print('|TRAIN| : {} |VALID| : {} |TEST| : {}'.format(train_df.shape, valid_df.shape, test_df.shape))\n",
    "    print(train_y.shape, valid_y.shape)\n",
    "    return train_df, valid_df, test_df, train_y, valid_y, sample\n",
    "\n",
    "\n",
    "def autoencoding(train_df, test_df, TARGET=1, verbose=2) :\n",
    "    TRAIN_LENGTH = 1205\n",
    "\n",
    "    df = pd.concat([train_df, test_df], axis=0)\n",
    "    text_df = df[['조식메뉴', '중식메뉴', '석식메뉴']]\n",
    "\n",
    "    mecab = Mecab(dicpath='C:/mecab/mecab-ko-dic')\n",
    "\n",
    "    for i in range(len(text_df.조식메뉴)) :\n",
    "        text_df.조식메뉴[i] = ','.join(text_df.조식메뉴[i].split())\n",
    "        text_df.조식메뉴[i] = ' '.join(mecab.morphs(text_df.조식메뉴[i]))\n",
    "    for i in range(len(text_df.중식메뉴)) :\n",
    "        text_df.중식메뉴[i] = ','.join(text_df.중식메뉴[i].split())\n",
    "        text_df.중식메뉴[i] = ' '.join(mecab.morphs(text_df.중식메뉴[i]))\n",
    "    for i in range(len(text_df.석식메뉴)) :\n",
    "        text_df.석식메뉴[i] = ','.join(text_df.석식메뉴[i].split())\n",
    "        text_df.석식메뉴[i] = ' '.join(mecab.morphs(text_df.석식메뉴[i]))\n",
    "\n",
    "    vect = CountVectorizer()\n",
    "    vect.fit(text_df['조식메뉴'].values[:TRAIN_LENGTH])\n",
    "    breakfast = vect.transform(text_df['조식메뉴'].values).toarray()\n",
    "    vect.fit(text_df['중식메뉴'].values[:TRAIN_LENGTH])\n",
    "    lunch = vect.transform(text_df['중식메뉴'].values).toarray()\n",
    "    vect.fit(text_df['석식메뉴'].values[:TRAIN_LENGTH])\n",
    "    dinner = vect.transform(text_df['석식메뉴'].values).toarray()\n",
    "\n",
    "    enc_df = pd.DataFrame()\n",
    "\n",
    "    i = 1\n",
    "    for menu in [breakfast, lunch, dinner] :\n",
    "        print('+' * 10, 'Train {}'.format(i), '+' * 10)\n",
    "        train_X, valid_X = train_test_split(menu[:TRAIN_LENGTH],\n",
    "                                            test_size=0.1,\n",
    "                                            shuffle=True,\n",
    "                                            random_state=0)\n",
    "        STEP = (menu.shape[1] - TARGET) // 5\n",
    "        INPUT = menu.shape[1]\n",
    "\n",
    "        model = AutoEncoder(INPUT, STEP)\n",
    "        model.compile(loss='mse', optimizer='adam', metrics='mse')\n",
    "        model.fit(train_X, train_X,\n",
    "                  validation_data=(valid_X, valid_X),\n",
    "                  epochs=1000,\n",
    "                  callbacks=tf.keras.callbacks.EarlyStopping(patience=10),\n",
    "                  verbose=verbose)\n",
    "        result = model.encoder(menu)\n",
    "        result = np.array(result)\n",
    "\n",
    "        enc_df = pd.concat([enc_df, pd.DataFrame(result)], axis=1)\n",
    "        i += 1\n",
    "        print()\n",
    "\n",
    "    train_df.drop(columns=['조식메뉴', '중식메뉴', '석식메뉴'], inplace=True)\n",
    "    test_df.drop(columns=['조식메뉴', '중식메뉴', '석식메뉴'], inplace=True)\n",
    "\n",
    "    train_df_enc = pd.concat([train_df, enc_df[:TRAIN_LENGTH]], axis=1)\n",
    "    test_df_enc = pd.concat([test_df, enc_df[TRAIN_LENGTH :]], axis=1)\n",
    "\n",
    "    column_names = list(train_df_enc.columns[:8]) + [i for i in range(enc_df.shape[1])]\n",
    "    train_df_enc.columns = column_names\n",
    "    test_df_enc.columns = column_names\n",
    "\n",
    "    return train_df_enc, test_df_enc\n",
    "\n",
    "\n",
    "def get_data(config) :\n",
    "    train_df, valid_df, test_df, train_y, valid_y, sample = process(config)\n",
    "    if config.text == 'autoencode' :\n",
    "        train_df, test_df = autoencoding(train_df, test_df, TARGET=config.dim)\n",
    "\n",
    "    return train_df, valid_df, test_df, train_y, valid_y, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train.csv', encoding='utf-8')\n",
    "test_df = pd.read_csv('../data/test.csv', encoding='utf-8')\n",
    "sample = pd.read_csv('../data/sample_submission.csv', encoding='utf-8')\n",
    "\n",
    "y = train_df[['중식계', '석식계']]\n",
    "\n",
    "TRAIN_LENGTH = 1205\n",
    "\n",
    "df = pd.concat([train_df, test_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['공휴일전후'] = 0\n",
    "    df['공휴일전후'][17] = 1\n",
    "    df['공휴일전후'][4] = 1\n",
    "    df\n",
    "    df['공휴일전후'][62] = 1\n",
    "    df['공휴일전후'][131] = 1\n",
    "    df['공휴일전후'][152] = 1\n",
    "    df['공휴일전후'][226] = 1\n",
    "    df['공휴일전후'][221] = 1\n",
    "    df['공휴일전후'][224] = 1\n",
    "    df['공휴일전후'][245] = 1\n",
    "    df['공휴일전후'][310] = 2\n",
    "    df['공휴일전후'][311] = 1\n",
    "    df['공휴일전후'][309] = 1\n",
    "    df['공휴일전후'][330] = 1\n",
    "    df['공휴일전후'][379] = 1\n",
    "    df['공휴일전후'][467] = 1\n",
    "    df['공휴일전후'][470] = 1\n",
    "    df['공휴일전후'][502] = 2\n",
    "    df['공휴일전후'][565] = 1\n",
    "    df['공휴일전후'][623] = 1\n",
    "    df['공휴일전후'][651] = 1\n",
    "    df['공휴일전후'][705] = 1\n",
    "    df['공휴일전후'][709] = 1\n",
    "    df['공휴일전후'][815] = 1\n",
    "    df['공휴일전후'][864] = 1\n",
    "    df['공휴일전후'][950] = 1\n",
    "    df['공휴일전후'][951] = 1\n",
    "    df['공휴일전후'][953] = 1\n",
    "    df['공휴일전후'][955] = 1\n",
    "    df['공휴일전후'][954] = 1\n",
    "    df['공휴일전후'][971] = 1\n",
    "    df['공휴일전후'][1038] = 1\n",
    "    df['공휴일전후'][1099] = 1\n",
    "    df['공휴일전후'][1129] = 1\n",
    "    df['공휴일전후'][1187] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-03-02'"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['일자'].iloc[21] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-09\n"
     ]
    }
   ],
   "source": [
    "train_df['공휴일전후'] = 0\n",
    "train_df['공휴일전후'][17] = 1 # 삼일절 전\n",
    "train_df['공휴일전후'][6] = 2 # 설 연휴 전\n",
    "train_df['공휴일전후'][47] = 1 # 총선 전\n",
    "train_df['공휴일전후'][62] = 4 # 어린이 날 전\n",
    "train_df['공휴일전후'][82] = 4 # 현충일 연휴 전(금요일)\n",
    "train_df['공휴일전후'][131] = 3 # 광복절 연휴 전(금요일)\n",
    "train_df['공휴일전후'][152] = 5 # 한가위 연휴 전\n",
    "train_df['공휴일전후'][162] = 3 # 개천절 연휴 전(금요일)\n",
    "train_df['공휴일전후'][226] = 2 # 연말\n",
    "train_df['공휴일전후'][221] = 2 # 크리스마스 전(금요일)\n",
    "train_df['공휴일전후'][245] = 4 # 설 연휴 전\n",
    "train_df['공휴일전후'][310] = 3 # 어린이 날 연휴 전(금요일)\n",
    "train_df['공휴일전후'][309] = 1 # 석가탄신일 전\n",
    "train_df['공휴일전후'][330] = 1 # 현충일 전\n",
    "train_df['공휴일전후'][379] = 1 # 광복절 전\n",
    "train_df['공휴일전후'][412] = 11 # 추석연휴 전\n",
    "train_df['공휴일전후'][466] = 3 # 성탄절 연휴 전(금요일)\n",
    "train_df['공휴일전후'][470] = 3 # 연말\n",
    "train_df['공휴일전후'][502] = 4 # 설 연휴 전\n",
    "train_df['공휴일전후'][510] = 3 # 삼일절 전\n",
    "train_df['공휴일전후'][565] = 1 # 석가탄신일 전\n",
    "train_df['공휴일전후'][575] = 1 # 현충일 전\n",
    "train_df['공휴일전후'][623] = 1 # 광복절 전\n",
    "train_df['공휴일전후'][650] = 2 # 추석 연휴 전\n",
    "train_df['공휴일전후'][651] = 16 # 한글날 전\n",
    "train_df['공휴일전후'][705] = 1 # 크리스마스 이브\n",
    "train_df['공휴일전후'][709] = 1 # 연말\n",
    "train_df['공휴일전후'][732] = 5 # 설연휴 전\n",
    "train_df['공휴일전후'][748] = 3 # 삼일절 연휴 전\n",
    "train_df['공휴일전후'][792] = 3 # 어린이날 연휴 전\n",
    "train_df['공휴일전후'][814] = 1 # 현충일 전\n",
    "train_df['공휴일전후'][863] = 1 # 광복절 전\n",
    "train_df['공휴일전후'][882] = 4 # 추석 연휴 전\n",
    "train_df['공휴일전후'][894] = 1 # 개천절 전\n",
    "train_df['공휴일전후'][897] = 1 # 한글날 전\n",
    "train_df['공휴일전후'][951] = 1 # 크리스마스 전\n",
    "train_df['공휴일전후'][955] = 1 # 연말\n",
    "train_df['공휴일전후'][971] = 4 # 설 연휴\n",
    "train_df['공휴일전후'][1027] = 1 # 국회의원선거 전\n",
    "train_df['공휴일전후'][1037] = 4 # 석가탄신일 연휴 전\n",
    "train_df['공휴일전후'][1038] = 1 # 어린이날 전\n",
    "train_df['공휴일전후'][1129] = 7 # 추석연휴 전\n",
    "train_df['공휴일전후'][1133] = 3 # 한글날 연휴 전\n",
    "train_df['공휴일전후'][1187] = 10 # 성탄절 연휴 전\n",
    "\n",
    "test_df['공휴일전후'][10] = 4 # 설연휴전\n",
    "test_df['공휴일전후'][20] = 3 # 삼일절 연휴 전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from argparse import Namespace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import fasttext\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "\n",
    "TRAIN_LENGTH = 1205\n",
    "\n",
    "\n",
    "def pretext(text) :\n",
    "    text = text.rstrip()\n",
    "    text = re.sub('▁', '', text)\n",
    "    return text\n",
    "\n",
    "def get_fasttext_model(config, train_df, save=False):\n",
    "    print('\"{}\" DOES NOT EXIST. START TRAINING MODEL'.format(config.fasttext_model_fn))\n",
    "    corpus = np.concatenate([train_df.조식메뉴.values,\n",
    "                             train_df.중식메뉴.values,\n",
    "                             train_df.석식메뉴.values], axis=0)\n",
    "    with open('../data/corpus.train.txt', 'w', -1, encoding='utf-8') as f :\n",
    "        f.write('\\n'.join(corpus))\n",
    "\n",
    "    if config.dummy_corpus :\n",
    "        args = {'load_fn' : '../data/corpus.train.txt',\n",
    "                'save_fn' : './data/corpus.train.dummy.txt',\n",
    "                'iter' : 2,\n",
    "                'verbose' : 300}\n",
    "        args = Namespace(**args)\n",
    "        main(args)\n",
    "\n",
    "    model = fasttext.train_unsupervised(\n",
    "        '../data/corpus.train.dummy.txt' if config.dummy_corpus else '../data/corpus.train.txt',\n",
    "        dim=config.dim,\n",
    "        ws=config.window_size,\n",
    "        epoch=config.fasttext_epoch,\n",
    "        min_count=config.min_count,\n",
    "        minn=config.min_ngram,\n",
    "        maxn=config.max_ngram,\n",
    "        )\n",
    "    if save:\n",
    "        model.save_model('./data/{}'.format(config.fasttext_model_fn))\n",
    "    return model\n",
    "\n",
    "def embedding(config, train_df, valid_df, test_df):\n",
    "    embedding_features = []\n",
    "\n",
    "    if config.pretrained:\n",
    "        print('USE PRETRAINED MODEL: cc.ko.300.bin')\n",
    "        model = fasttext.load_model('cc.ko.300.bin')\n",
    "        fasttext.util.reduce_model(model, config.dim)\n",
    "\n",
    "    else:\n",
    "        if config.fasttext_model_fn is not None:\n",
    "            try:\n",
    "                model = fasttext.load_model('../data/{}'.format(config.fasttext_model_fn))\n",
    "            except:\n",
    "                model = get_fasttext_model(config, train_df, save=True)\n",
    "        else:\n",
    "            model = get_fasttext_model(config, train_df, save=False)\n",
    "\n",
    "\n",
    "    TRAIN_LENGTH, VALID_LENGTH = train_df.shape[0], valid_df.shape[0]\n",
    "\n",
    "    df = pd.concat([train_df, valid_df, test_df], axis=0)\n",
    "    breakfast = df.조식메뉴.values\n",
    "    lunch = df.중식메뉴.values\n",
    "    dinner = df.석식메뉴.values\n",
    "\n",
    "    breakfast_array = np.zeros((1255, config.dim))\n",
    "    lunch_array = np.zeros((1255, config.dim))\n",
    "    dinner_array = np.zeros((1255, config.dim))\n",
    "    for i in range(1255) :\n",
    "\n",
    "        breakfast_array[i] = model.get_sentence_vector(breakfast[i]) / (len(breakfast[i].split()) + 1)\n",
    "        lunch_array[i] = model.get_sentence_vector(lunch[i]) / (len(lunch[i].split()) + 1)\n",
    "        dinner_array[i] = model.get_sentence_vector(dinner[i]) / (len(dinner[i].split()) + 1)\n",
    "\n",
    "    for i in range(config.dim) :\n",
    "        embedding_features.append('breakfast_{}'.format(i))\n",
    "        embedding_features.append('lunch_{}'.format(i))\n",
    "        embedding_features.append('dinner_{}'.format(i))\n",
    "\n",
    "    tmp = pd.concat([\n",
    "        pd.DataFrame(breakfast_array, columns=['breakfast_{}'.format(i) for i in range(config.dim)]),\n",
    "        pd.DataFrame(lunch_array, columns=['lunch_{}'.format(i) for i in range(config.dim)]),\n",
    "        pd.DataFrame(dinner_array, columns=['dinner_{}'.format(i) for i in range(config.dim)])], axis=1)\n",
    "    train_tmp = tmp[:TRAIN_LENGTH]\n",
    "    valid_tmp = tmp[TRAIN_LENGTH: TRAIN_LENGTH + VALID_LENGTH]\n",
    "    test_tmp = tmp[TRAIN_LENGTH + VALID_LENGTH:]\n",
    "\n",
    "    return train_tmp, valid_tmp, test_tmp\n",
    "\n",
    "# def subword(config):\n",
    "#     # corpus = '\\n'.join(np.concatenate([breakfast[:TRAIN_LENGTH],\n",
    "#     #                                    lunch[:TRAIN_LENGTH],\n",
    "#     #                                    dinner[:TRAIN_LENGTH]], axis=0))\n",
    "#     # with open('./data/corpus.train.txt', 'w', encoding='utf-8') as f:\n",
    "#     #     f.write(corpus)\n",
    "#     #\n",
    "#     # corpus = '\\n'.join(np.concatenate([breakfast[TRAIN_LENGTH:],\n",
    "#     #                                    lunch[TRAIN_LENGTH:],\n",
    "#     #                                    dinner[TRAIN_LENGTH:]], axis=0))\n",
    "#     # with open('./data/corpus.test.txt', 'w', encoding='utf-8') as f:\n",
    "#     #     f.write(corpus)\n",
    "#\n",
    "#     # =================== subword segment=========================\n",
    "#     with open('./data/corpus.train.sub.txt', 'r', encoding='utf-8') as f :\n",
    "#         data = f.readlines()\n",
    "#     corpus_sub_train = list(map(pretext, data))\n",
    "#     with open('./data/corpus.valid.sub.txt', 'r', encoding='utf-8') as f :\n",
    "#         data = f.readlines()\n",
    "#     corpus_sub_valid = list(map(pretext, data))\n",
    "#     with open('./data/corpus.test.sub.txt', 'r', encoding='utf-8') as f :\n",
    "#         data = f.readlines()\n",
    "#     corpus_sub_test = list(map(pretext, data))\n",
    "#\n",
    "#     tokenizer = Tokenizer(oov_token='<oov>')\n",
    "#     tokenizer.fit_on_texts(corpus_sub_train)\n",
    "#\n",
    "#     vocab_size = len(tokenizer.word_index)\n",
    "#     print('VOCAB SIZE : {}'.format(vocab_size))\n",
    "#     cnt = 0\n",
    "#     for i in tokenizer.word_counts.values():\n",
    "#         if i == 1:\n",
    "#             cnt += 1\n",
    "#     print('Freq 1 word : {}'.format(cnt))\n",
    "#     tokenizer = Tokenizer(oov_token='<oov>', num_words=vocab_size \\\n",
    "#         if not config.sub_sparse_word \\\n",
    "#         else vocab_size - cnt + 2)\n",
    "#\n",
    "#     tokenizer.fit_on_texts(corpus_sub_train)\n",
    "#\n",
    "#     corpus_sub = corpus_sub_train + corpus_sub_valid + corpus_sub_test\n",
    "#     corpus_seq = tokenizer.texts_to_sequences(corpus_sub)\n",
    "#\n",
    "#     embeds = nn.Embedding(839, config.dim)\n",
    "#\n",
    "#     def get_sentence_vect(seq) :\n",
    "#         result = torch.tensor(torch.zeros((config.dim)))\n",
    "#         for idx in seq :\n",
    "#             result += embeds(torch.tensor(idx, dtype=torch.long))\n",
    "#         return (result / len(seq)).detach().numpy()\n",
    "#\n",
    "#     tmp = pd.DataFrame(np.array(list(map(get_sentence_vect, corpus_seq))))\n",
    "#     tmp = pd.concat([tmp.iloc[:1255, :],\n",
    "#                      tmp.iloc[1255 :2510, :].reset_index(drop=True),\n",
    "#                      tmp.iloc[2510 :, :].reset_index(drop=True)], axis=1)\n",
    "#     columns = []\n",
    "#\n",
    "#     for menu in ['breakfast', 'lunch', 'dinner']:\n",
    "#         for i in range(config.dim):\n",
    "#             columns.append('{}_{}'.format(menu, i))\n",
    "#\n",
    "#     tmp.columns = columns\n",
    "#\n",
    "#     return tmp\n",
    "\n",
    "class Encoder(tf.keras.models.Model) :\n",
    "    def __init__(self, step, input_size) :\n",
    "        super(Encoder, self).__init__()\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "            InputLayer(input_shape=(input_size,)),\n",
    "            Dense(input_size - step * 1, activation='relu'),\n",
    "            Dense(input_size - step * 2, activation='relu'),\n",
    "            Dense(input_size - step * 3, activation='relu'),\n",
    "            Dense(input_size - step * 4, activation='relu'),\n",
    "            Dense(input_size - step * 5),\n",
    "        ])\n",
    "\n",
    "    def call(self, x) :\n",
    "        z = self.model(x)\n",
    "        return z\n",
    "\n",
    "class Decoder(tf.keras.models.Model) :\n",
    "    def __init__(self, step, input_size, output_size) :\n",
    "        super(Decoder, self).__init__()\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "            InputLayer(input_shape=(input_size,)),\n",
    "            Dense(output_size - step * 4, activation='relu'),\n",
    "            #             Dense(output_size - step * 3, activation='relu'),\n",
    "            Dense(output_size - step * 2, activation='relu'),\n",
    "            #             Dense(output_size - step * 1, activation='relu'),\n",
    "            Dense(output_size),\n",
    "        ])\n",
    "\n",
    "    def call(self, x) :\n",
    "        z = self.model(x)\n",
    "        return z\n",
    "\n",
    "class AutoEncoder(tf.keras.models.Model) :\n",
    "    def __init__(self, input_size, step) :\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder(step, input_size)\n",
    "        self.decoder = Decoder(step, input_size - step * 5, input_size)\n",
    "\n",
    "    def call(self, x) :\n",
    "        y = self.encoder(x)\n",
    "        z = self.decoder(y)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|TRAIN| : (1000, 25) |VALID| : (205, 25) |TEST| : (50, 25)\n",
      "(1000, 2) (205, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "config = {'temp':True,\n",
    "          'sep_date':True,\n",
    "          'text':'embedding',\n",
    "          'pretrained':False,\n",
    "          'fasttext_model_fn':'basemodel.bin',\n",
    "          'dim':3,\n",
    "          'dummy_corpus':False,          \n",
    "         }\n",
    "config = Namespace(**config)\n",
    "train_df, valid_df, test_df, train_y, valid_y, sample = get_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>요일</th>\n",
       "      <th>본사시간외근무명령서승인건수</th>\n",
       "      <th>출근</th>\n",
       "      <th>휴가비율</th>\n",
       "      <th>출장비율</th>\n",
       "      <th>야근비율</th>\n",
       "      <th>재택비율</th>\n",
       "      <th>공휴일전후_0</th>\n",
       "      <th>공휴일전후_1</th>\n",
       "      <th>공휴일전후_2</th>\n",
       "      <th>...</th>\n",
       "      <th>week</th>\n",
       "      <th>breakfast_0</th>\n",
       "      <th>breakfast_1</th>\n",
       "      <th>breakfast_2</th>\n",
       "      <th>lunch_0</th>\n",
       "      <th>lunch_1</th>\n",
       "      <th>lunch_2</th>\n",
       "      <th>dinner_0</th>\n",
       "      <th>dinner_1</th>\n",
       "      <th>dinner_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.284483</td>\n",
       "      <td>0.637185</td>\n",
       "      <td>0.035955</td>\n",
       "      <td>0.676305</td>\n",
       "      <td>0.288445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.001252</td>\n",
       "      <td>-0.043469</td>\n",
       "      <td>0.076748</td>\n",
       "      <td>-0.006939</td>\n",
       "      <td>-0.097955</td>\n",
       "      <td>0.045741</td>\n",
       "      <td>-0.079515</td>\n",
       "      <td>-0.099580</td>\n",
       "      <td>-0.036920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.399425</td>\n",
       "      <td>0.453841</td>\n",
       "      <td>0.304542</td>\n",
       "      <td>0.379229</td>\n",
       "      <td>0.460419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.001589</td>\n",
       "      <td>-0.039055</td>\n",
       "      <td>0.070557</td>\n",
       "      <td>0.009484</td>\n",
       "      <td>-0.084852</td>\n",
       "      <td>0.023651</td>\n",
       "      <td>-0.061752</td>\n",
       "      <td>-0.108893</td>\n",
       "      <td>-0.044417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.371648</td>\n",
       "      <td>0.597159</td>\n",
       "      <td>0.046141</td>\n",
       "      <td>0.818139</td>\n",
       "      <td>0.386995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.011059</td>\n",
       "      <td>-0.046916</td>\n",
       "      <td>0.068368</td>\n",
       "      <td>0.032917</td>\n",
       "      <td>-0.106392</td>\n",
       "      <td>0.039426</td>\n",
       "      <td>-0.065591</td>\n",
       "      <td>-0.089481</td>\n",
       "      <td>-0.038905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.630084</td>\n",
       "      <td>0.019829</td>\n",
       "      <td>0.695691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.007148</td>\n",
       "      <td>-0.044558</td>\n",
       "      <td>0.073010</td>\n",
       "      <td>-0.006174</td>\n",
       "      <td>-0.108710</td>\n",
       "      <td>0.016387</td>\n",
       "      <td>0.002914</td>\n",
       "      <td>-0.104132</td>\n",
       "      <td>0.035737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.605552</td>\n",
       "      <td>0.169656</td>\n",
       "      <td>0.758222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.006174</td>\n",
       "      <td>-0.044015</td>\n",
       "      <td>0.064435</td>\n",
       "      <td>-0.103627</td>\n",
       "      <td>-0.081909</td>\n",
       "      <td>-0.025678</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>-0.102112</td>\n",
       "      <td>0.026354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>3</td>\n",
       "      <td>0.286398</td>\n",
       "      <td>0.631375</td>\n",
       "      <td>0.045799</td>\n",
       "      <td>0.567868</td>\n",
       "      <td>0.291500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003574</td>\n",
       "      <td>-0.048775</td>\n",
       "      <td>0.068870</td>\n",
       "      <td>0.016007</td>\n",
       "      <td>-0.104690</td>\n",
       "      <td>0.052849</td>\n",
       "      <td>-0.013133</td>\n",
       "      <td>-0.087281</td>\n",
       "      <td>0.018353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.708844</td>\n",
       "      <td>0.046401</td>\n",
       "      <td>0.796513</td>\n",
       "      <td>0.484182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.002929</td>\n",
       "      <td>-0.044858</td>\n",
       "      <td>0.075416</td>\n",
       "      <td>0.035288</td>\n",
       "      <td>-0.098378</td>\n",
       "      <td>0.053732</td>\n",
       "      <td>-0.087552</td>\n",
       "      <td>-0.074172</td>\n",
       "      <td>-0.015849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>3</td>\n",
       "      <td>0.426245</td>\n",
       "      <td>0.670110</td>\n",
       "      <td>0.079857</td>\n",
       "      <td>0.440906</td>\n",
       "      <td>0.423037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.006900</td>\n",
       "      <td>-0.043013</td>\n",
       "      <td>0.076566</td>\n",
       "      <td>0.002836</td>\n",
       "      <td>-0.105239</td>\n",
       "      <td>0.050572</td>\n",
       "      <td>0.020192</td>\n",
       "      <td>-0.088211</td>\n",
       "      <td>0.048113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>0.126437</td>\n",
       "      <td>0.583602</td>\n",
       "      <td>0.119600</td>\n",
       "      <td>0.926483</td>\n",
       "      <td>0.132873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.002530</td>\n",
       "      <td>-0.043074</td>\n",
       "      <td>0.076762</td>\n",
       "      <td>0.012102</td>\n",
       "      <td>-0.103406</td>\n",
       "      <td>0.041513</td>\n",
       "      <td>0.017983</td>\n",
       "      <td>-0.110493</td>\n",
       "      <td>0.013947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.628793</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>0.707374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.003852</td>\n",
       "      <td>-0.048005</td>\n",
       "      <td>0.072567</td>\n",
       "      <td>-0.028440</td>\n",
       "      <td>-0.092691</td>\n",
       "      <td>-0.010535</td>\n",
       "      <td>0.008709</td>\n",
       "      <td>-0.105519</td>\n",
       "      <td>0.045370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     요일  본사시간외근무명령서승인건수        출근      휴가비율      출장비율      야근비율  재택비율  \\\n",
       "0     4        0.284483  0.637185  0.035955  0.676305  0.288445   0.0   \n",
       "1     1        0.399425  0.453841  0.304542  0.379229  0.460419   0.0   \n",
       "2     4        0.371648  0.597159  0.046141  0.818139  0.386995   0.0   \n",
       "3     2        0.000000  0.630084  0.019829  0.695691  0.000000   0.0   \n",
       "4     0        0.000000  0.605552  0.169656  0.758222  0.000000   0.0   \n",
       "..   ..             ...       ...       ...       ...       ...   ...   \n",
       "995   3        0.286398  0.631375  0.045799  0.567868  0.291500   0.0   \n",
       "996   1        0.500000  0.708844  0.046401  0.796513  0.484182   0.0   \n",
       "997   3        0.426245  0.670110  0.079857  0.440906  0.423037   0.0   \n",
       "998   0        0.126437  0.583602  0.119600  0.926483  0.132873   0.0   \n",
       "999   2        0.000000  0.628793  0.016528  0.707374  0.000000   0.0   \n",
       "\n",
       "     공휴일전후_0  공휴일전후_1  공휴일전후_2  ...  week  breakfast_0  breakfast_1  \\\n",
       "0          1        0        0  ...     3    -0.001252    -0.043469   \n",
       "1          1        0        0  ...     1    -0.001589    -0.039055   \n",
       "2          1        0        0  ...     2    -0.011059    -0.046916   \n",
       "3          1        0        0  ...     1    -0.007148    -0.044558   \n",
       "4          1        0        0  ...     2    -0.006174    -0.044015   \n",
       "..       ...      ...      ...  ...   ...          ...          ...   \n",
       "995        1        0        0  ...     4     0.003574    -0.048775   \n",
       "996        1        0        0  ...     4    -0.002929    -0.044858   \n",
       "997        1        0        0  ...     1    -0.006900    -0.043013   \n",
       "998        1        0        0  ...     2    -0.002530    -0.043074   \n",
       "999        1        0        0  ...     3    -0.003852    -0.048005   \n",
       "\n",
       "     breakfast_2   lunch_0   lunch_1   lunch_2  dinner_0  dinner_1  dinner_2  \n",
       "0       0.076748 -0.006939 -0.097955  0.045741 -0.079515 -0.099580 -0.036920  \n",
       "1       0.070557  0.009484 -0.084852  0.023651 -0.061752 -0.108893 -0.044417  \n",
       "2       0.068368  0.032917 -0.106392  0.039426 -0.065591 -0.089481 -0.038905  \n",
       "3       0.073010 -0.006174 -0.108710  0.016387  0.002914 -0.104132  0.035737  \n",
       "4       0.064435 -0.103627 -0.081909 -0.025678  0.001615 -0.102112  0.026354  \n",
       "..           ...       ...       ...       ...       ...       ...       ...  \n",
       "995     0.068870  0.016007 -0.104690  0.052849 -0.013133 -0.087281  0.018353  \n",
       "996     0.075416  0.035288 -0.098378  0.053732 -0.087552 -0.074172 -0.015849  \n",
       "997     0.076566  0.002836 -0.105239  0.050572  0.020192 -0.088211  0.048113  \n",
       "998     0.076762  0.012102 -0.103406  0.041513  0.017983 -0.110493  0.013947  \n",
       "999     0.072567 -0.028440 -0.092691 -0.010535  0.008709 -0.105519  0.045370  \n",
       "\n",
       "[1000 rows x 25 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"basemodel.bin\" DOES NOT EXIST. START TRAINING MODEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/corpus.train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-403f491236e6>\u001b[0m in \u001b[0;36membedding\u001b[1;34m(config, train_df, valid_df, test_df)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfasttext_model_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\fasttext\\FastText.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[0meprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_FastText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\fasttext\\FastText.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_path, args)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: ./data/basemodel.bin cannot be opened for loading!",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-9a97fe589cb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m          }\n\u001b[0;32m      9\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNamespace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mvalid_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalid_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-c50ac0b821fa>\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m     \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'autoencode'\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautoencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTARGET\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-c50ac0b821fa>\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'embedding'\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mtrain_tmp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_tmp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_tmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;31m# if config.text == 'subword':\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-403f491236e6>\u001b[0m in \u001b[0;36membedding\u001b[1;34m(config, train_df, valid_df, test_df)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfasttext_model_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_fasttext_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_fasttext_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-403f491236e6>\u001b[0m in \u001b[0;36mget_fasttext_model\u001b[1;34m(config, train_df, save)\u001b[0m\n\u001b[0;32m     25\u001b[0m                              \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m중식메뉴\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                              train_df.석식메뉴.values], axis=0)\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/corpus.train.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/corpus.train.txt'"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "config = {'temp':True,\n",
    "          'sep_date':True,\n",
    "          'text':'embedding',\n",
    "          'pretrained':False,\n",
    "          'fasttext_model_fn':'basemodel.bin',\n",
    "          'dim':3,\n",
    "         }\n",
    "config = Namespace(**config)\n",
    "train_df, valid_df, test_df, train_y, valid_y, sample = get_data(config)\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_y.reset_index(drop=True)], axis=0)\n",
    "valid_df = pd.concat([valid_df.reset_index(drop=True), valid_y.reset_index(drop=True)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column type forced is either target column or doesn't exist in the dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-ccfffc540d95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mFOLD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m reg = setup(data=train_df.drop(columns=['석식계']), target='중식계', test_data=valid_df.drop(columns=['석식계']),\n\u001b[0m\u001b[0;32m      5\u001b[0m             categorical_features=[0, 7, 8, 9, 12, 13, 14, 15], fold=FOLD)\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pycaret\\regression.py\u001b[0m in \u001b[0;36msetup\u001b[1;34m(data, target, train_size, test_data, preprocess, imputation_type, iterative_imputation_iters, categorical_features, categorical_imputation, categorical_iterative_imputer, ordinal_features, high_cardinality_features, high_cardinality_method, numeric_features, numeric_imputation, numeric_iterative_imputer, date_features, ignore_features, normalize, normalize_method, transformation, transformation_method, handle_unknown_categorical, unknown_categorical_method, pca, pca_method, pca_components, ignore_low_variance, combine_rare_levels, rare_level_threshold, bin_numeric_features, remove_outliers, outliers_threshold, remove_multicollinearity, multicollinearity_threshold, remove_perfect_collinearity, create_clusters, cluster_iter, polynomial_features, polynomial_degree, trigonometry_features, polynomial_threshold, group_features, group_names, feature_selection, feature_selection_threshold, feature_selection_method, feature_interaction, feature_ratio, interaction_threshold, transform_target, transform_target_method, data_split_shuffle, data_split_stratify, fold_strategy, fold, fold_shuffle, fold_groups, n_jobs, use_gpu, custom_pipeline, html, session_id, log_experiment, experiment_name, log_plots, log_profile, log_data, silent, verbose, profile)\u001b[0m\n\u001b[0;32m    563\u001b[0m         \u001b[0mlog_plots\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"residuals\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"error\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"feature\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m     return pycaret.internal.tabular.setup(\n\u001b[0m\u001b[0;32m    566\u001b[0m         \u001b[0mml_usecase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"regression\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[0mavailable_plots\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mavailable_plots\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pycaret\\internal\\tabular.py\u001b[0m in \u001b[0;36msetup\u001b[1;34m(data, target, ml_usecase, available_plots, train_size, test_data, preprocess, imputation_type, iterative_imputation_iters, categorical_features, categorical_imputation, categorical_iterative_imputer, ordinal_features, high_cardinality_features, high_cardinality_method, numeric_features, numeric_imputation, numeric_iterative_imputer, date_features, ignore_features, normalize, normalize_method, transformation, transformation_method, handle_unknown_categorical, unknown_categorical_method, pca, pca_method, pca_components, ignore_low_variance, combine_rare_levels, rare_level_threshold, bin_numeric_features, remove_outliers, outliers_threshold, remove_multicollinearity, multicollinearity_threshold, remove_perfect_collinearity, create_clusters, cluster_iter, polynomial_features, polynomial_degree, trigonometry_features, polynomial_threshold, group_features, group_names, feature_selection, feature_selection_threshold, feature_selection_method, feature_interaction, feature_ratio, interaction_threshold, fix_imbalance, fix_imbalance_method, transform_target, transform_target_method, data_split_shuffle, data_split_stratify, fold_strategy, fold, fold_shuffle, fold_groups, n_jobs, use_gpu, custom_pipeline, html, session_id, log_experiment, experiment_name, log_plots, log_profile, log_data, silent, verbose, profile, display)\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_cols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    593\u001b[0m                     \u001b[1;34m\"Column type forced is either target column or doesn't exist in the dataset.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m                 )\n",
      "\u001b[1;31mValueError\u001b[0m: Column type forced is either target column or doesn't exist in the dataset."
     ]
    }
   ],
   "source": [
    "from pycaret.regression import *\n",
    "\n",
    "FOLD = 10\n",
    "reg = setup(data=train_df.drop(columns=['석식계']), target='중식계', test_data=valid_df.drop(columns=['석식계']),\n",
    "            categorical_features=[0, 7, 8, 9, 12, 13, 14, 15], fold=FOLD)\n",
    "\n",
    "print('=' * 10, 'Comparing models...', '=' * 10)\n",
    "best5 = compare_models(fold=FOLD, sort='mae', n_select=5)\n",
    "\n",
    "print('=' * 10, 'Tuning models', '=' * 10)\n",
    "tuned_best5 = [tune_model(i, optimize='mae', \n",
    "                          early_stopping=True, \n",
    "                          early_stopping_max_iters=100,\n",
    "                          choose_better=True) for i in best5]\n",
    "print('=' * 10, 'Blending...', '=' * 10)\n",
    "blend_best5 = blend_models(estimator_list=tuned_best5, fold=FOLD, optimize='mae',\n",
    "                           choose_better=True)\n",
    "\n",
    "print('=' * 10, 'Finalizing...', '=' * 10)\n",
    "pred = predict_model(blend_best5)\n",
    "final_model = finalize_model(blend_best5)\n",
    "pred = predict_model(final_model, data=test_df)\n",
    "print('=' * 10, 'Completed!', '=' * 10)\n",
    "sample['중식계'] = pred.Label.values\n",
    "\n",
    "\n",
    "reg = setup(data=train_df.drop(columns=['중식계']), target='석식계', test_data=valid_df.drop(columns=['중식계']),\n",
    "            categorical_features=[0, 7, 8, 9, 12, 13, 14, 15], fold=FOLD)\n",
    "\n",
    "print('=' * 10, 'Comparing models...', '=' * 10)\n",
    "best5 = compare_models(fold=FOLD, sort='mae', n_select=5)\n",
    "\n",
    "print('=' * 10, 'Tuning models', '=' * 10)\n",
    "tuned_best5 = [tune_model(i, optimize='mae', \n",
    "                          early_stopping=True, \n",
    "                          early_stopping_max_iters=100,\n",
    "                          choose_better=True) for i in best5]\n",
    "print('=' * 10, 'Blending...', '=' * 10)\n",
    "blend_best5 = blend_models(estimator_list=tuned_best5, fold=FOLD, optimize='mae',\n",
    "                           choose_better=True)\n",
    "\n",
    "print('=' * 10, 'Finalizing...', '=' * 10)\n",
    "pred = predict_model(blend_best5)\n",
    "final_model = finalize_model(blend_best5)\n",
    "pred = predict_model(final_model, data=test_df)\n",
    "print('=' * 10, 'Completed!', '=' * 10)\n",
    "sample['석식계'] = pred.Label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv('submission_pycaret.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submission_pycaret.csv\n",
    "- 5dim fasttext / n_select 2\n",
    "- 10dim fasttext / n_select 3 / normalize / multicollinearity ths 0.9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
